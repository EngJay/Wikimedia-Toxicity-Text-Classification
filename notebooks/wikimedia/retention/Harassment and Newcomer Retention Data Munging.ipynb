{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import statsmodels.formula.api as sm\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find harassed editors\n",
    "Any editor who ever made an edit and received a harassing comment by another user on their user page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9131778, 2)\n",
      "9131778\n"
     ]
    }
   ],
   "source": [
    "df_all_users = pd.read_csv(\"../../data/retention/user_start.tsv\", \"\\t\")\n",
    "print(df_all_users.shape)\n",
    "print(len(df_all_users.user_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9119417, 2)\n",
      "(9110021, 2)\n"
     ]
    }
   ],
   "source": [
    "# load data for resolving user pagetitles to user_ids\n",
    "# only works for ns=user\n",
    "\n",
    "df_i2ns = pd.read_csv(\"../../data/retention/user_id_to_names.tsv\", \"\\t\")\n",
    "print(df_i2ns.shape)\n",
    "# can't deal with different ids taking on the same username at differnt times\n",
    "df_i2ns = df_i2ns.drop_duplicates(\"user_text\")\n",
    "print(df_i2ns.shape)\n",
    "df_i2ns = df_i2ns.rename(columns={'user_id': 'to_user_id', 'user_text': 'to_user_text'})\n",
    "\n",
    "def resolve_page_title(df):\n",
    "    df['to_user_text'] = df['to_user_text'].apply(lambda x: str(x).split(\"/\")[0])\n",
    "    df = df.merge(df_i2ns, how = \"left\", on = \"to_user_text\")\n",
    "    del df['to_user_text']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(58, 1)\n",
      "(350, 1)\n",
      "(1205, 1)\n",
      "(4470, 1)\n",
      "(14678, 1)\n",
      "(17473, 1)\n",
      "(14282, 1)\n",
      "(10519, 1)\n",
      "(8304, 1)\n",
      "(6521, 1)\n",
      "(6481, 1)\n",
      "(5648, 1)\n",
      "(5825, 1)\n",
      "(5401, 1)\n",
      "Num atttacked pages:  27690\n",
      "27690\n",
      "Num atttacked users:  27690\n"
     ]
    }
   ],
   "source": [
    "# get unique set of user talk pages with an attack\n",
    "usecols = [0, 3,5,6,8,9,11,12,13]\n",
    "years = range(2001,2016)\n",
    "threshold = 0.425\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "\n",
    "        df = pd.read_csv(\"../../data/figshare/scored/comments_user_%d.tsv.gz\" % year,\n",
    "                         sep = \"\\t\",\n",
    "                         compression = \"gzip\",\n",
    "                         usecols = usecols)\n",
    "        \n",
    "        df = df.query(\"bot == 0 and admin == 0\")\n",
    "        \n",
    "        df = df.rename(columns={'user_id': 'from_user_id', \n",
    "                                'user_text': 'from_user_text', \n",
    "                                'page_title': 'to_user_text'})\n",
    "        \n",
    "        df = df.query(\"pred_attack_score > %f \\\n",
    "                       or pred_aggression_score > %f \\\n",
    "                       or pred_toxicity_score > %f\" % (threshold, threshold, threshold))\n",
    "        \n",
    "        df = df[['to_user_text', 'from_user_id']]\n",
    "        \n",
    "        # get to_user_id\n",
    "        df = resolve_page_title(df).dropna()\n",
    "        # remove comments by user on own page\n",
    "        df = df.query(\"from_user_id != to_user_id\")\n",
    "        # grab ids of attacked users\n",
    "        df = df[['to_user_id']]\n",
    "        \n",
    "        dfs.append(df)\n",
    "        print(df.shape)\n",
    "        \n",
    "        \n",
    "df_attacked_users = pd.concat(dfs).drop_duplicates()\n",
    "df_attacked_users.columns = ['user_id']\n",
    "print(\"Num atttacked pages: \", df_attacked_users.shape[0])\n",
    "\n",
    "# get user start dates\n",
    "df_attacked_users = df_attacked_users.merge(df_all_users, on = 'user_id')\n",
    "df_attacked_users['first_edit_day'] = pd.to_datetime(df_attacked_users['first_edit_day'], format = '%Y%m%d')\n",
    "df_attacked_users = df_attacked_users.dropna()\n",
    "print( df_attacked_users.shape[0])\n",
    "\n",
    "# save df\n",
    "df_attacked_users['sample'] = \"attacked\"\n",
    "df_attacked_users.to_csv(\"../../data/retention/attacked_users.csv\", index = False)\n",
    "print(\"Num atttacked users: \", df_attacked_users.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random User Sample\n",
    "\n",
    "Random sample of users who made at least one edit at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size:  100000\n"
     ]
    }
   ],
   "source": [
    "n_random = 100000\n",
    "df_random_users = df_all_users.sample(n_random, random_state = 12)\n",
    "df_random_users['sample'] = \"random\"\n",
    "df_random_users['first_edit_day'] = pd.to_datetime(df_all_users['first_edit_day'], format = '%Y%m%d')\n",
    "df_random_users = df_random_users.dropna()\n",
    "df_random_users.to_csv(\"../../data/retention/random_users.csv\", index = False)\n",
    "print(\"Sample Size: \", df_random_users.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load user history data for newcomer sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The data used in this analysis includes:\n",
    "1. all user and article talk page comments, labeled by harassment classifiers, except those generated by bots or templates\n",
    "2. all newly registered users, who made at least one edit\n",
    "3. edits per day per namespace for all newcomers\n",
    "4. user warnings received by 2015 newcomers in\n",
    "5. genders of all editors if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sample_users = pd.concat([df_random_users, df_attacked_users]).drop_duplicates()\n",
    "df_sample_users[\"last_day\"] = df_sample_users[\"first_edit_day\"] + pd.Timedelta('186 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2354123\n"
     ]
    }
   ],
   "source": [
    "# get comments from users in sample for first 6 months\n",
    "nss = ['user', 'article']\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    for ns in nss:\n",
    "\n",
    "        df = pd.read_csv(\"../../data/figshare/scored/comments_%s_%d.tsv.gz\" % (ns, year),\n",
    "                         sep = \"\\t\",\n",
    "                         compression = \"gzip\",\n",
    "                         usecols = usecols)\n",
    "        \n",
    "        df = df.query(\"bot == 0 and admin == 0\")\n",
    "        \n",
    "        df = df.rename(columns={'user_id': 'from_user_id', \n",
    "                                'user_text': 'from_user_text', \n",
    "                                'page_title': 'to_user_text'})\n",
    "        \n",
    "        df['ns'] = ns\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        if ns == \"user\":\n",
    "            df = resolve_page_title(df)\n",
    "        else:\n",
    "            df['to_user_id'] = -1\n",
    "            del df['to_user_text']\n",
    "            \n",
    "        df = df.query(\"from_user_id != to_user_id\")\n",
    "        \n",
    "        \n",
    "        # comments made by users in the sample in 6 months since first edit\n",
    "        df = df.merge(df_sample_users[['user_id', 'last_day']], how = 'inner', left_on = \"from_user_id\", right_on = 'user_id')\n",
    "        del df['user_id']\n",
    "        df = df.query(\"timestamp < last_day\")\n",
    "        dfs.append(df)\n",
    "        \n",
    "        \n",
    "df_comments_from = pd.concat(dfs).drop_duplicates(\"rev_id\")\n",
    "del df_comments_from['last_day']\n",
    "print(df_comments_from.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832604\n"
     ]
    }
   ],
   "source": [
    "# get comments to users in sample for first 6 month\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    df = pd.read_csv(\"../../data/figshare/scored/comments_user_%d.tsv.gz\" %  year,\n",
    "                     sep = \"\\t\",\n",
    "                     compression = \"gzip\",\n",
    "                     usecols = usecols)\n",
    "\n",
    "    df = df.query(\"bot == 0 and admin == 0\")\n",
    "\n",
    "    df = df.rename(columns={'user_id': 'from_user_id', \n",
    "                            'user_text': 'from_user_text', \n",
    "                            'page_title': 'to_user_text'})\n",
    "\n",
    "    df['ns'] = 'user'\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = resolve_page_title(df)\n",
    "    df = df.query(\"from_user_id != to_user_id\")\n",
    "\n",
    "\n",
    "    df = df.merge(df_sample_users[['user_id','last_day']], how = 'inner', left_on = 'to_user_id', right_on = 'user_id')\n",
    "    del df['user_id']\n",
    "    df = df.query(\"timestamp < last_day\")\n",
    "    dfs.append(df)\n",
    "\n",
    "df_comments_to = pd.concat(dfs).drop_duplicates(\"rev_id\")\n",
    "del df_comments_to['last_day']\n",
    "print(df_comments_to.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92282221\n",
      "32422391\n",
      "3274798\n"
     ]
    }
   ],
   "source": [
    "# load edits per day for editors in sample\n",
    "df_edits = pd.read_csv(\"../../data/retention/daily_revision_counts.tsv\", \"\\t\")\n",
    "print(df_edits.shape[0])\n",
    "df_edits = df_edits.merge(df_sample_users, how = 'inner', on = 'user_id')\n",
    "df_edits['timestamp'] = pd.to_datetime(df_edits['day'].apply(lambda x: str(x)))\n",
    "print(df_edits.shape[0])\n",
    "df_edits = df_edits.query(\"timestamp < last_day\")\n",
    "print(df_edits.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39107\n"
     ]
    }
   ],
   "source": [
    "# load user warnings for editors in sample\n",
    "df_uw = pd.read_csv(\"../../data/retention/user_warnings.tsv\", \"\\t\")\n",
    "\n",
    "df_uw = df_uw.rename(columns={'user_id': 'from_user_id', \n",
    "                                'user_text': 'from_user_text', \n",
    "                                'page_title': 'to_user_text'})\n",
    "\n",
    "df_uw = resolve_page_title(df_uw)\n",
    "df_uw = df_uw.merge(df_sample_users[['user_id','last_day']], how = 'inner', left_on = 'to_user_id', right_on = 'user_id')\n",
    "del df_uw['user_id']\n",
    "df_uw['timestamp'] = pd.to_datetime(df_uw['rev_timestamp'])\n",
    "df_uw = df_uw.query(\"timestamp < last_day\")\n",
    "print(df_uw.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create df of consolidated user level features\n",
    "df_gender = pd.read_csv(\"../../data/misc/genders.tsv\", \"\\t\")[['user_id', 'gender']]\n",
    "df_user = df_sample_users.merge(df_gender, on = 'user_id', how = \"left\")\n",
    "df_user['gender'] = df_user['gender'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Pickle User Objects\n",
    "To be able help with extracting user level features for subsequent studies, we group data sources above by user and store the results in a dedicated `User` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map data frames into dictionaries keyed by user\n",
    "def gb_to_dict(gb):\n",
    "    return { i:k for i,k in gb}\n",
    "\n",
    "df_comments_from_groups = gb_to_dict(df_comments_from.groupby(\"from_user_id\"))\n",
    "df_comments_to_groups =  gb_to_dict(df_comments_to.query(\"ns == 'user'\").groupby(\"to_user_id\"))\n",
    "df_edits_groups =  gb_to_dict(df_edits.groupby(\"user_id\"))\n",
    "df_user_groups =  gb_to_dict(df_user.groupby(\"user_id\"))\n",
    "df_uw_groups =  gb_to_dict(df_uw.groupby(\"to_user_id\")) # page title is the recipient of the uw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect User objects \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from user_object import User\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attacked_user_objects = [User( user_id,\n",
    "                      df_comments_from_groups,\n",
    "                      df_comments_to_groups,\n",
    "                      df_edits_groups,\n",
    "                      df_user_groups, \n",
    "                      df_uw_groups) \n",
    "                for user_id in df_attacked_users['user_id']]\n",
    "\n",
    "pickle.dump(attacked_user_objects, open(\"../../data/retention/attacked_user_data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_user_objects = [User( user_id,\n",
    "                      df_comments_from_groups,\n",
    "                      df_comments_to_groups,\n",
    "                      df_edits_groups,\n",
    "                      df_user_groups, \n",
    "                      df_uw_groups) \n",
    "                for user_id in df_random_users['user_id']]\n",
    "\n",
    "pickle.dump(random_user_objects, open(\"../../data/retention/random_user_data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
