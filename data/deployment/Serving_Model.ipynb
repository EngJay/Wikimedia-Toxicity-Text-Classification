{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development for Serving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in id2word for decoding the encoded examples.\n",
    "f = BytesIO(file_io.read_file_to_string('Wikimedia-Toxicity-Personal-Attacks/output/Gao-PA-word2id.bin', binary_mode=True))\n",
    "id2word_dict = msgpack.unpack(f, raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_comment = \"`-NEWLINE_TOKENThis is not ``creative``.  Those are the dictionary definitions of the terms ``insurance`` and ``ensurance`` as properly applied to ``destruction``.  If you don't understand that, fine, legitimate criticism, I'll write up ``three man cell`` and ``bounty hunter`` and then it will be easy to understand why ``ensured`` and ``insured`` are different - and why both differ from ``assured``.NEWLINE_TOKENNEWLINE_TOKENThe sentence you quote is absolutely neutral.  You just aren't familiar with the underlying theory of strike-back (e.g. submarines as employed in nuclear warfare) guiding the insurance, nor likely the three man cell structure that kept the IRA from being broken by the British.  If that's my fault, fine, I can fix that to explain.  But ther'es nothing ``personal`` or ``creative`` about it.NEWLINE_TOKENNEWLINE_TOKENI'm tired of arguing with you.  Re: the other article, ``multi-party`` turns up plenty, and there is more use of ``mutually`` than ``mutual``.  If I were to apply your standard I'd be moving ``Mutual Assured Destruction`` to ``talk`` for not appealing to a Reagan voter's biases about its effectiveness, and for dropping the ``ly``.NEWLINE_TOKENNEWLINE_TOKENThere is a double standard in your edits.  If it comes from some US history book, like ``peace movement`` or 'M.A.D.' as defined in 1950, you like it, even if the definition is totally useless in 2002 and only of historical interest. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_text(text, encoding_dict):\n",
    "\n",
    "    # Clean text.\n",
    "\n",
    "    # Remove unwanted tokens.\n",
    "    text = re.sub('NEWLINE_TOKEN', ' ', text)\n",
    "    text = re.sub('TAB_TOKEN', ' ', text)\n",
    "\n",
    "    # Force lowercase.\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove single and double backticks.\n",
    "    text = re.sub(\"`\", '', text)\n",
    "\n",
    "    # Remove single quotes.\n",
    "    text = re.sub(\"'\", '', text)\n",
    "\n",
    "    # Replace multiple periods in sequence with one period.\n",
    "    text = re.sub(\"\\.{2,}\", '.', text)\n",
    "\n",
    "    # Replace everything except words, '.', '|', '?', and '!' with space.\n",
    "    text = re.sub('[^\\w_|\\.|\\?|!]+', ' ', text)\n",
    "\n",
    "    # Replace periods with ' . '.\n",
    "    text = re.sub('\\.', ' . ', text)\n",
    "\n",
    "    # Replace '?' with ' ? '.\n",
    "    text = re.sub('\\?', ' ? ', text)\n",
    "\n",
    "    # Replace '!' with ' ! '.\n",
    "    text = re.sub('!', ' ! ', text)\n",
    "\n",
    "    # Tokenize by splitting on whitespace.\n",
    "    # No leading or trailing whitespace is kept.\n",
    "    # Consecutive spaces are treated as a single space.\n",
    "    text = text.split()\n",
    "\n",
    "    # Split into sentences, store tokens, get max sentence length.\n",
    "    tokens = []\n",
    "    maxsentlen = 0\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for t in text:\n",
    "        # Use '.', '!', '?' as markers of end of sentence.\n",
    "        if t not in ['.', '!', '?']:\n",
    "            # Not at end of a sentence.\n",
    "            sentence.append(t)\n",
    "        else:\n",
    "            # At end of a sentence.\n",
    "            sentence.append(t)\n",
    "\n",
    "            # Add sentence to sentences.\n",
    "            sentences.append(sentence)\n",
    "\n",
    "            # Track longest sentence.\n",
    "            if len(sentence) > maxsentlen:\n",
    "                maxsentlen = len(sentence)\n",
    "\n",
    "            # Reset sentence list.\n",
    "            sentence = []\n",
    "\n",
    "    # If sentence has word, add to list of sentences.\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    # Add split sentences to tokens.\n",
    "    tokens.append(sentences)\n",
    "\n",
    "    # Index for unknown words.\n",
    "    unk = len(encoding_dict) - 1\n",
    "\n",
    "    # Convert words to word indices.\n",
    "    for idx, doc in enumerate(tokens):\n",
    "        # Build list of indicies representing the words of each sentence,\n",
    "        # if word is not a key in word2id mapping, use unk.\n",
    "        encoded_text = []\n",
    "        for sent in doc:\n",
    "            encoded_text.append(\n",
    "                [encoding_dict[word] if word in encoding_dict else unk for word in sent])\n",
    "    \n",
    "    # Flatten list of encoded words.\n",
    "    encoded_text = [item for sublist in encoded_text for item in sublist]\n",
    "\n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14,\n",
       " 9,\n",
       " 16,\n",
       " 3450,\n",
       " 1,\n",
       " 139,\n",
       " 21,\n",
       " 2,\n",
       " 2165,\n",
       " 2892,\n",
       " 5,\n",
       " 2,\n",
       " 730,\n",
       " 5838,\n",
       " 4,\n",
       " 36992,\n",
       " 20,\n",
       " 1216,\n",
       " 1939,\n",
       " 3,\n",
       " 3670,\n",
       " 1,\n",
       " 26,\n",
       " 7,\n",
       " 55,\n",
       " 252,\n",
       " 11,\n",
       " 724,\n",
       " 1362,\n",
       " 786,\n",
       " 237,\n",
       " 376,\n",
       " 79,\n",
       " 415,\n",
       " 406,\n",
       " 2347,\n",
       " 4,\n",
       " 23324,\n",
       " 6344,\n",
       " 4,\n",
       " 87,\n",
       " 13,\n",
       " 46,\n",
       " 19,\n",
       " 1159,\n",
       " 3,\n",
       " 252,\n",
       " 78,\n",
       " 21947,\n",
       " 4,\n",
       " 16460,\n",
       " 21,\n",
       " 272,\n",
       " 4,\n",
       " 78,\n",
       " 226,\n",
       " 5657,\n",
       " 36,\n",
       " 6684,\n",
       " 1,\n",
       " 2,\n",
       " 521,\n",
       " 7,\n",
       " 644,\n",
       " 9,\n",
       " 976,\n",
       " 487,\n",
       " 1,\n",
       " 7,\n",
       " 53,\n",
       " 749,\n",
       " 1546,\n",
       " 24,\n",
       " 2,\n",
       " 6033,\n",
       " 701,\n",
       " 5,\n",
       " 2972,\n",
       " 159,\n",
       " 334,\n",
       " 1,\n",
       " 705,\n",
       " 1,\n",
       " 24998,\n",
       " 20,\n",
       " 4923,\n",
       " 12,\n",
       " 3201,\n",
       " 5393,\n",
       " 15850,\n",
       " 2,\n",
       " 5838,\n",
       " 584,\n",
       " 769,\n",
       " 2,\n",
       " 415,\n",
       " 406,\n",
       " 2347,\n",
       " 2042,\n",
       " 11,\n",
       " 1339,\n",
       " 2,\n",
       " 4196,\n",
       " 36,\n",
       " 93,\n",
       " 2141,\n",
       " 33,\n",
       " 2,\n",
       " 574,\n",
       " 1,\n",
       " 26,\n",
       " 178,\n",
       " 30,\n",
       " 2655,\n",
       " 724,\n",
       " 8,\n",
       " 43,\n",
       " 923,\n",
       " 11,\n",
       " 3,\n",
       " 490,\n",
       " 1,\n",
       " 29,\n",
       " 424,\n",
       " 224,\n",
       " 189,\n",
       " 28,\n",
       " 3450,\n",
       " 39,\n",
       " 13,\n",
       " 1,\n",
       " 77,\n",
       " 2381,\n",
       " 5,\n",
       " 1986,\n",
       " 24,\n",
       " 7,\n",
       " 1,\n",
       " 389,\n",
       " 2,\n",
       " 66,\n",
       " 27,\n",
       " 3786,\n",
       " 557,\n",
       " 3463,\n",
       " 79,\n",
       " 1996,\n",
       " 4,\n",
       " 44,\n",
       " 9,\n",
       " 64,\n",
       " 98,\n",
       " 5,\n",
       " 7517,\n",
       " 97,\n",
       " 4829,\n",
       " 1,\n",
       " 26,\n",
       " 8,\n",
       " 83,\n",
       " 3,\n",
       " 1281,\n",
       " 23,\n",
       " 846,\n",
       " 333,\n",
       " 19,\n",
       " 1541,\n",
       " 4829,\n",
       " 6684,\n",
       " 3670,\n",
       " 3,\n",
       " 67,\n",
       " 15,\n",
       " 16,\n",
       " 5434,\n",
       " 3,\n",
       " 6,\n",
       " 7773,\n",
       " 5797,\n",
       " 5295,\n",
       " 39,\n",
       " 50,\n",
       " 7518,\n",
       " 4,\n",
       " 15,\n",
       " 5525,\n",
       " 2,\n",
       " 24999,\n",
       " 1,\n",
       " 44,\n",
       " 9,\n",
       " 6,\n",
       " 1551,\n",
       " 846,\n",
       " 12,\n",
       " 23,\n",
       " 121,\n",
       " 1,\n",
       " 26,\n",
       " 13,\n",
       " 788,\n",
       " 36,\n",
       " 65,\n",
       " 201,\n",
       " 181,\n",
       " 332,\n",
       " 49,\n",
       " 1619,\n",
       " 1225,\n",
       " 28,\n",
       " 794,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 434,\n",
       " 1,\n",
       " 20,\n",
       " 1921,\n",
       " 12,\n",
       " 6685,\n",
       " 7,\n",
       " 49,\n",
       " 13,\n",
       " 95,\n",
       " 26,\n",
       " 2,\n",
       " 727,\n",
       " 9,\n",
       " 964,\n",
       " 1079,\n",
       " 12,\n",
       " 2331,\n",
       " 4,\n",
       " 82,\n",
       " 5,\n",
       " 849,\n",
       " 541,\n",
       " 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encode_text(text_comment, id2word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
