{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Lexicon of Abusive Words (GitHub handle: uds-lsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import string\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import msgpack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible_noun</td>\n",
       "      <td>3.679601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgusting_adj</td>\n",
       "      <td>3.493682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>moron_noun</td>\n",
       "      <td>3.469677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bastard_noun</td>\n",
       "      <td>3.399238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stupid_noun</td>\n",
       "      <td>3.323882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1\n",
       "0   horrible_noun  3.679601\n",
       "1  disgusting_adj  3.493682\n",
       "2      moron_noun  3.469677\n",
       "3    bastard_noun  3.399238\n",
       "4     stupid_noun  3.323882"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in lexicon.\n",
    "abusive_words_df = pd.read_csv('lexicons_archives/lexicon-of-abusive-words/Lexicons/expandedLexicon.txt', sep='\\t', header=None)\n",
    "abusive_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.936462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.919384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  1.000000\n",
       "1  0.957866\n",
       "2  0.952425\n",
       "3  0.936462\n",
       "4  0.919384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the values to [-1, 1]\n",
    "x = abusive_words_df[[1]].values # Returns a numpy array.\n",
    "min = -1\n",
    "max = 1\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(min, max))\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "normalized_abusive_words_df = pd.DataFrame(x_scaled)\n",
    "normalized_abusive_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>abusive_lex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible_noun</td>\n",
       "      <td>3.679601</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgusting_adj</td>\n",
       "      <td>3.493682</td>\n",
       "      <td>0.957866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>moron_noun</td>\n",
       "      <td>3.469677</td>\n",
       "      <td>0.952425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bastard_noun</td>\n",
       "      <td>3.399238</td>\n",
       "      <td>0.936462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stupid_noun</td>\n",
       "      <td>3.323882</td>\n",
       "      <td>0.919384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1  abusive_lex\n",
       "0   horrible_noun  3.679601     1.000000\n",
       "1  disgusting_adj  3.493682     0.957866\n",
       "2      moron_noun  3.469677     0.952425\n",
       "3    bastard_noun  3.399238     0.936462\n",
       "4     stupid_noun  3.323882     0.919384"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge dataframes.\n",
    "abusive_words_joined_df = abusive_words_df.join(normalized_abusive_words_df, how='outer', rsuffix='_norm')\n",
    "abusive_words_joined_df.columns = ['0', '1', 'abusive_lex']\n",
    "abusive_words_joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>abusive_lex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible_noun</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgusting_adj</td>\n",
       "      <td>0.957866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>moron_noun</td>\n",
       "      <td>0.952425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bastard_noun</td>\n",
       "      <td>0.936462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stupid_noun</td>\n",
       "      <td>0.919384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  abusive_lex\n",
       "0   horrible_noun     1.000000\n",
       "1  disgusting_adj     0.957866\n",
       "2      moron_noun     0.952425\n",
       "3    bastard_noun     0.936462\n",
       "4     stupid_noun     0.919384"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop non-normalized column.\n",
    "abusive_words_joined_norm_df = abusive_words_joined_df.drop('1', axis=1)\n",
    "abusive_words_joined_norm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>abusive_lex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible_noun</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0  abusive_lex\n",
       "0  horrible_noun          1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# abusive_words_joined_norm_df['0'].where(abusive_words_joined_norm_df['0'] == 'horrible_noun')\n",
    "abusive_words_joined_norm_df[abusive_words_joined_norm_df['0'] == 'horrible_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>abusive_lex</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible_noun</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgusting_adj</td>\n",
       "      <td>0.957866</td>\n",
       "      <td>disgusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>moron_noun</td>\n",
       "      <td>0.952425</td>\n",
       "      <td>moron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bastard_noun</td>\n",
       "      <td>0.936462</td>\n",
       "      <td>bastard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stupid_noun</td>\n",
       "      <td>0.919384</td>\n",
       "      <td>stupid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  abusive_lex        word\n",
       "0   horrible_noun     1.000000    horrible\n",
       "1  disgusting_adj     0.957866  disgusting\n",
       "2      moron_noun     0.952425       moron\n",
       "3    bastard_noun     0.936462     bastard\n",
       "4     stupid_noun     0.919384      stupid"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply lambda to remove POS labels.\n",
    "words_df = abusive_words_joined_norm_df['0'].apply(lambda x: x.replace('_noun', '').replace('_adj', '').replace('_verb', ''))\n",
    "abusive_words_joined_norm_df['word'] = words_df\n",
    "abusive_words_joined_norm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>abusive_lex</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stupid_adj</td>\n",
       "      <td>0.889470</td>\n",
       "      <td>stupid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>horrible_adj</td>\n",
       "      <td>0.887407</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bastard_adj</td>\n",
       "      <td>0.844285</td>\n",
       "      <td>bastard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bitch_verb</td>\n",
       "      <td>0.743532</td>\n",
       "      <td>bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>slut_verb</td>\n",
       "      <td>0.731548</td>\n",
       "      <td>slut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>stink_verb</td>\n",
       "      <td>0.706906</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>crap_verb</td>\n",
       "      <td>0.697943</td>\n",
       "      <td>crap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>rubbish_adj</td>\n",
       "      <td>0.696167</td>\n",
       "      <td>rubbish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>crap_adj</td>\n",
       "      <td>0.685363</td>\n",
       "      <td>crap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>fuck_verb</td>\n",
       "      <td>0.673528</td>\n",
       "      <td>fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>rubbish_verb</td>\n",
       "      <td>0.643928</td>\n",
       "      <td>rubbish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>revolting_adj</td>\n",
       "      <td>0.639233</td>\n",
       "      <td>revolting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>bugger_verb</td>\n",
       "      <td>0.631058</td>\n",
       "      <td>bugger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>prank_verb</td>\n",
       "      <td>0.625337</td>\n",
       "      <td>prank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>shit_adj</td>\n",
       "      <td>0.625198</td>\n",
       "      <td>shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>weird_verb</td>\n",
       "      <td>0.624198</td>\n",
       "      <td>weird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>stinking_adj</td>\n",
       "      <td>0.621022</td>\n",
       "      <td>stinking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>fart_verb</td>\n",
       "      <td>0.611657</td>\n",
       "      <td>fart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weird_adj</td>\n",
       "      <td>0.607734</td>\n",
       "      <td>weird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>snot_verb</td>\n",
       "      <td>0.604404</td>\n",
       "      <td>snot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>scum_verb</td>\n",
       "      <td>0.603182</td>\n",
       "      <td>scum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>piss_verb</td>\n",
       "      <td>0.597203</td>\n",
       "      <td>piss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>lunatic_adj</td>\n",
       "      <td>0.591628</td>\n",
       "      <td>lunatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>twat_verb</td>\n",
       "      <td>0.588026</td>\n",
       "      <td>twat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>shit_verb</td>\n",
       "      <td>0.580177</td>\n",
       "      <td>shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>fucking_adj</td>\n",
       "      <td>0.578873</td>\n",
       "      <td>fucking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>loony_adj</td>\n",
       "      <td>0.571712</td>\n",
       "      <td>loony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>tyrant_adj</td>\n",
       "      <td>0.571220</td>\n",
       "      <td>tyrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>phony_adj</td>\n",
       "      <td>0.563783</td>\n",
       "      <td>phony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>cruel_verb</td>\n",
       "      <td>0.560256</td>\n",
       "      <td>cruel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8404</th>\n",
       "      <td>tense_verb</td>\n",
       "      <td>-0.553498</td>\n",
       "      <td>tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>tense_adj</td>\n",
       "      <td>-0.561727</td>\n",
       "      <td>tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>lag_adj</td>\n",
       "      <td>-0.564692</td>\n",
       "      <td>lag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>unfamiliar_adj</td>\n",
       "      <td>-0.567287</td>\n",
       "      <td>unfamiliar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418</th>\n",
       "      <td>bias_adj</td>\n",
       "      <td>-0.569884</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>strike_verb</td>\n",
       "      <td>-0.570420</td>\n",
       "      <td>strike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8421</th>\n",
       "      <td>lag_verb</td>\n",
       "      <td>-0.573035</td>\n",
       "      <td>lag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>overestimate_verb</td>\n",
       "      <td>-0.578414</td>\n",
       "      <td>overestimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8425</th>\n",
       "      <td>conflict_noun</td>\n",
       "      <td>-0.581506</td>\n",
       "      <td>conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8426</th>\n",
       "      <td>divide_verb</td>\n",
       "      <td>-0.582178</td>\n",
       "      <td>divide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8427</th>\n",
       "      <td>bias_verb</td>\n",
       "      <td>-0.585164</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8428</th>\n",
       "      <td>strain_verb</td>\n",
       "      <td>-0.585683</td>\n",
       "      <td>strain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8431</th>\n",
       "      <td>embargo_verb</td>\n",
       "      <td>-0.587423</td>\n",
       "      <td>embargo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8432</th>\n",
       "      <td>slump_verb</td>\n",
       "      <td>-0.588015</td>\n",
       "      <td>slump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8433</th>\n",
       "      <td>decrease_verb</td>\n",
       "      <td>-0.588687</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8435</th>\n",
       "      <td>stress_verb</td>\n",
       "      <td>-0.591320</td>\n",
       "      <td>stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8436</th>\n",
       "      <td>concern_verb</td>\n",
       "      <td>-0.602727</td>\n",
       "      <td>concern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>problem_noun</td>\n",
       "      <td>-0.620041</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8444</th>\n",
       "      <td>dispute_verb</td>\n",
       "      <td>-0.622060</td>\n",
       "      <td>dispute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8450</th>\n",
       "      <td>delay_verb</td>\n",
       "      <td>-0.638593</td>\n",
       "      <td>delay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8453</th>\n",
       "      <td>hurdle_verb</td>\n",
       "      <td>-0.644417</td>\n",
       "      <td>hurdle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8460</th>\n",
       "      <td>stir_verb</td>\n",
       "      <td>-0.664605</td>\n",
       "      <td>stir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8461</th>\n",
       "      <td>regret_verb</td>\n",
       "      <td>-0.666531</td>\n",
       "      <td>regret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8462</th>\n",
       "      <td>reason_verb</td>\n",
       "      <td>-0.668614</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>decline_verb</td>\n",
       "      <td>-0.712260</td>\n",
       "      <td>decline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>trouble_verb</td>\n",
       "      <td>-0.716252</td>\n",
       "      <td>trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8471</th>\n",
       "      <td>struggle_verb</td>\n",
       "      <td>-0.747811</td>\n",
       "      <td>struggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8472</th>\n",
       "      <td>debate_verb</td>\n",
       "      <td>-0.772704</td>\n",
       "      <td>debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8475</th>\n",
       "      <td>tension_noun</td>\n",
       "      <td>-0.833014</td>\n",
       "      <td>tension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8477</th>\n",
       "      <td>doubt_verb</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>doubt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1429 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0  abusive_lex          word\n",
       "7            stupid_adj     0.889470        stupid\n",
       "8          horrible_adj     0.887407      horrible\n",
       "11          bastard_adj     0.844285       bastard\n",
       "25           bitch_verb     0.743532         bitch\n",
       "32            slut_verb     0.731548          slut\n",
       "41           stink_verb     0.706906         stink\n",
       "43            crap_verb     0.697943          crap\n",
       "44          rubbish_adj     0.696167       rubbish\n",
       "53             crap_adj     0.685363          crap\n",
       "63            fuck_verb     0.673528          fuck\n",
       "80         rubbish_verb     0.643928       rubbish\n",
       "83        revolting_adj     0.639233     revolting\n",
       "94          bugger_verb     0.631058        bugger\n",
       "96           prank_verb     0.625337         prank\n",
       "97             shit_adj     0.625198          shit\n",
       "98           weird_verb     0.624198         weird\n",
       "100        stinking_adj     0.621022      stinking\n",
       "111           fart_verb     0.611657          fart\n",
       "115           weird_adj     0.607734         weird\n",
       "118           snot_verb     0.604404          snot\n",
       "121           scum_verb     0.603182          scum\n",
       "125           piss_verb     0.597203          piss\n",
       "129         lunatic_adj     0.591628       lunatic\n",
       "131           twat_verb     0.588026          twat\n",
       "141           shit_verb     0.580177          shit\n",
       "142         fucking_adj     0.578873       fucking\n",
       "150           loony_adj     0.571712         loony\n",
       "152          tyrant_adj     0.571220        tyrant\n",
       "160           phony_adj     0.563783         phony\n",
       "163          cruel_verb     0.560256         cruel\n",
       "...                 ...          ...           ...\n",
       "8404         tense_verb    -0.553498         tense\n",
       "8408          tense_adj    -0.561727         tense\n",
       "8412            lag_adj    -0.564692           lag\n",
       "8415     unfamiliar_adj    -0.567287    unfamiliar\n",
       "8418           bias_adj    -0.569884          bias\n",
       "8419        strike_verb    -0.570420        strike\n",
       "8421           lag_verb    -0.573035           lag\n",
       "8424  overestimate_verb    -0.578414  overestimate\n",
       "8425      conflict_noun    -0.581506      conflict\n",
       "8426        divide_verb    -0.582178        divide\n",
       "8427          bias_verb    -0.585164          bias\n",
       "8428        strain_verb    -0.585683        strain\n",
       "8431       embargo_verb    -0.587423       embargo\n",
       "8432         slump_verb    -0.588015         slump\n",
       "8433      decrease_verb    -0.588687      decrease\n",
       "8435        stress_verb    -0.591320        stress\n",
       "8436       concern_verb    -0.602727       concern\n",
       "8443       problem_noun    -0.620041       problem\n",
       "8444       dispute_verb    -0.622060       dispute\n",
       "8450         delay_verb    -0.638593         delay\n",
       "8453        hurdle_verb    -0.644417        hurdle\n",
       "8460          stir_verb    -0.664605          stir\n",
       "8461        regret_verb    -0.666531        regret\n",
       "8462        reason_verb    -0.668614        reason\n",
       "8465       decline_verb    -0.712260       decline\n",
       "8466       trouble_verb    -0.716252       trouble\n",
       "8471      struggle_verb    -0.747811      struggle\n",
       "8472        debate_verb    -0.772704        debate\n",
       "8475       tension_noun    -0.833014       tension\n",
       "8477         doubt_verb    -1.000000         doubt\n",
       "\n",
       "[1429 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem: 1429 words are duplicates/triplicates if POS label removed.\n",
    "abusive_words_joined_norm_df[abusive_words_joined_norm_df.duplicated('word')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>abusive_lex</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible_noun</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgusting_adj</td>\n",
       "      <td>0.957866</td>\n",
       "      <td>disgusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>moron_noun</td>\n",
       "      <td>0.952425</td>\n",
       "      <td>moron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bastard_noun</td>\n",
       "      <td>0.936462</td>\n",
       "      <td>bastard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stupid_noun</td>\n",
       "      <td>0.919384</td>\n",
       "      <td>stupid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  abusive_lex        word\n",
       "0   horrible_noun     1.000000    horrible\n",
       "1  disgusting_adj     0.957866  disgusting\n",
       "2      moron_noun     0.952425       moron\n",
       "3    bastard_noun     0.936462     bastard\n",
       "4     stupid_noun     0.919384      stupid"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df with duplicates dropped, retaining only the first occurance.\n",
    "abusive_words_joined_norm_first_occ_df = abusive_words_joined_norm_df.drop_duplicates('word').copy()\n",
    "abusive_words_joined_norm_first_occ_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "abusive_words_joined_norm_first_occ_df.drop('0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abusive_lex</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957866</td>\n",
       "      <td>disgusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952425</td>\n",
       "      <td>moron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.936462</td>\n",
       "      <td>bastard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.919384</td>\n",
       "      <td>stupid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.905426</td>\n",
       "      <td>bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.890157</td>\n",
       "      <td>scumbag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.864109</td>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.852809</td>\n",
       "      <td>idiot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.830330</td>\n",
       "      <td>slut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.814809</td>\n",
       "      <td>vile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.800816</td>\n",
       "      <td>redneck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.794813</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.779582</td>\n",
       "      <td>cretin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.776744</td>\n",
       "      <td>sadist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.768731</td>\n",
       "      <td>crap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.763229</td>\n",
       "      <td>sociopath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.753426</td>\n",
       "      <td>arent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.751187</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.746674</td>\n",
       "      <td>revolting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.746672</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.745000</td>\n",
       "      <td>bigot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.741470</td>\n",
       "      <td>brat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.739017</td>\n",
       "      <td>scum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.737779</td>\n",
       "      <td>crappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.736985</td>\n",
       "      <td>stinky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.731959</td>\n",
       "      <td>rubbish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.731743</td>\n",
       "      <td>fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.731280</td>\n",
       "      <td>liar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.726184</td>\n",
       "      <td>filthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8430</th>\n",
       "      <td>-0.586903</td>\n",
       "      <td>emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8434</th>\n",
       "      <td>-0.591039</td>\n",
       "      <td>stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8437</th>\n",
       "      <td>-0.602776</td>\n",
       "      <td>objection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8438</th>\n",
       "      <td>-0.606770</td>\n",
       "      <td>diminish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439</th>\n",
       "      <td>-0.607119</td>\n",
       "      <td>crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8440</th>\n",
       "      <td>-0.609551</td>\n",
       "      <td>limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8441</th>\n",
       "      <td>-0.614841</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8442</th>\n",
       "      <td>-0.616173</td>\n",
       "      <td>emotional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8445</th>\n",
       "      <td>-0.622288</td>\n",
       "      <td>hurdle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8446</th>\n",
       "      <td>-0.624236</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8447</th>\n",
       "      <td>-0.625541</td>\n",
       "      <td>unmoved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8448</th>\n",
       "      <td>-0.628091</td>\n",
       "      <td>delay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449</th>\n",
       "      <td>-0.634413</td>\n",
       "      <td>argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8451</th>\n",
       "      <td>-0.638675</td>\n",
       "      <td>indefinite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452</th>\n",
       "      <td>-0.642923</td>\n",
       "      <td>regret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8454</th>\n",
       "      <td>-0.646145</td>\n",
       "      <td>argue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8455</th>\n",
       "      <td>-0.650152</td>\n",
       "      <td>constrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8456</th>\n",
       "      <td>-0.650261</td>\n",
       "      <td>struggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8457</th>\n",
       "      <td>-0.651110</td>\n",
       "      <td>trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8458</th>\n",
       "      <td>-0.659296</td>\n",
       "      <td>decline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8459</th>\n",
       "      <td>-0.664063</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>-0.674235</td>\n",
       "      <td>downturn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464</th>\n",
       "      <td>-0.690782</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8467</th>\n",
       "      <td>-0.721949</td>\n",
       "      <td>disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8468</th>\n",
       "      <td>-0.724025</td>\n",
       "      <td>difficulty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8469</th>\n",
       "      <td>-0.732864</td>\n",
       "      <td>constraint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>-0.744171</td>\n",
       "      <td>debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8473</th>\n",
       "      <td>-0.779127</td>\n",
       "      <td>recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8474</th>\n",
       "      <td>-0.819965</td>\n",
       "      <td>tension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>-0.995637</td>\n",
       "      <td>doubt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7049 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abusive_lex          word\n",
       "0        1.000000      horrible\n",
       "1        0.957866    disgusting\n",
       "2        0.952425         moron\n",
       "3        0.936462       bastard\n",
       "4        0.919384        stupid\n",
       "5        0.905426         bitch\n",
       "6        0.890157       scumbag\n",
       "9        0.864109           ass\n",
       "10       0.852809         idiot\n",
       "12       0.830330          slut\n",
       "13       0.814809          vile\n",
       "14       0.800816       redneck\n",
       "15       0.794813         filth\n",
       "16       0.779582        cretin\n",
       "17       0.776744        sadist\n",
       "18       0.768731          crap\n",
       "19       0.763229     sociopath\n",
       "20       0.753426         arent\n",
       "21       0.751187         stink\n",
       "22       0.746674     revolting\n",
       "23       0.746672        rotten\n",
       "24       0.745000         bigot\n",
       "26       0.741470          brat\n",
       "27       0.739017          scum\n",
       "28       0.737779        crappy\n",
       "29       0.736985        stinky\n",
       "30       0.731959       rubbish\n",
       "31       0.731743          fuck\n",
       "33       0.731280          liar\n",
       "34       0.726184        filthy\n",
       "...           ...           ...\n",
       "8430    -0.586903       emotion\n",
       "8434    -0.591039        stress\n",
       "8437    -0.602776     objection\n",
       "8438    -0.606770      diminish\n",
       "8439    -0.607119        crisis\n",
       "8440    -0.609551       limited\n",
       "8441    -0.614841       problem\n",
       "8442    -0.616173     emotional\n",
       "8445    -0.622288        hurdle\n",
       "8446    -0.624236        reason\n",
       "8447    -0.625541       unmoved\n",
       "8448    -0.628091         delay\n",
       "8449    -0.634413      argument\n",
       "8451    -0.638675    indefinite\n",
       "8452    -0.642923        regret\n",
       "8454    -0.646145         argue\n",
       "8455    -0.650152     constrain\n",
       "8456    -0.650261      struggle\n",
       "8457    -0.651110       trouble\n",
       "8458    -0.659296       decline\n",
       "8459    -0.664063     uncertain\n",
       "8463    -0.674235      downturn\n",
       "8464    -0.690782    disappoint\n",
       "8467    -0.721949  disagreement\n",
       "8468    -0.724025    difficulty\n",
       "8469    -0.732864    constraint\n",
       "8470    -0.744171        debate\n",
       "8473    -0.779127      recourse\n",
       "8474    -0.819965       tension\n",
       "8476    -0.995637         doubt\n",
       "\n",
       "[7049 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abusive_words_joined_norm_first_occ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set words as index.\n",
    "abusive_words_joined_norm_first_occ_df.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dict.\n",
    "abusive_words_joined_norm_first_occ_dict = abusive_words_joined_norm_first_occ_df.to_dict()\n",
    "# abusive_words_joined_norm_first_occ_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in messagepack format.\n",
    "path = 'abusive-words-lex-first-occ.bin'\n",
    "with open(path, 'wb') as f:\n",
    "    msgpack.pack(abusive_words_joined_norm_first_occ_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
